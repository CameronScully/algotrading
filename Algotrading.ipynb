{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning headline analysis to predict daily stock movement\n",
    "## Goal\n",
    "- When should we buy and sell stocks for profit\n",
    "\n",
    "## Stratergy\n",
    "- What links are there between social media / news and stock movements?\n",
    "- Sentimate analysis of social media / news\n",
    "- Map Sentimate analysis to stock movement\n",
    "- Highlight stocks with high correlation\n",
    "- Train prediction models on those stocks\n",
    "\n",
    "## Approaches\n",
    "1. Bag-of-words machine learning model trained on historical data\n",
    "2. Spark clustering/continues learning trained on historical data\n",
    "3. mix of sources (reddit, twitter, abc)\n",
    "4. combination of headlines and traditional algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, pred):\n",
    "    print('---------------------------------------')\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_test, pred))\n",
    "    \n",
    "    #confusion matrix\n",
    "    print(\"Confussion Matrix: \")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "    \n",
    "    #Precision, recall, f-measures\n",
    "    print(\"Precision, recall, f-measures: \")\n",
    "    print(metrics.classification_report(y_test, pred))\n",
    "    \n",
    "    #Balanced accuracy\n",
    "    print(\"Balanced accuracy: \", metrics.balanced_accuracy_score(y_test, pred))\n",
    "    print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Machine learning\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#historical abc new headlines\n",
    "abc = pd.read_csv('abcnews-date-text.csv')\n",
    "\n",
    "#convert dates to match\n",
    "abc['publish_date'] = pd.to_datetime(abc['publish_date'], format='%Y%m%d')\n",
    "abc.rename(columns = {'publish_date':'Date'}, inplace=True)\n",
    "\n",
    "#combine all headlines on the same day\n",
    "abc = abc.groupby(['Date'])['headline_text'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "0.5253936722273945\n"
     ]
    }
   ],
   "source": [
    "tickers = pd.read_excel('Tickers.xlsx')\n",
    "\n",
    "#filtered_tickers = tickers[tickers['Country'] == 'Australia']\n",
    "filtered_tickers = tickers\n",
    "\n",
    "count = 0\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "pool = Pool(processes=12)\n",
    "pool.map(f, (ticker in filtered_tickers['Ticker']))\n",
    "\n",
    "for ticker in filtered_tickers['Ticker']:\n",
    "    #1. process stock\n",
    "    stock = pd.read_csv('Data/Data/'+ticker+'/'+ticker+'.csv')\n",
    "    \n",
    "    if stock.empty:\n",
    "        continue\n",
    "        \n",
    "    stock = stock.dropna()\n",
    "    \n",
    "    if stock.shape[0] < 100:\n",
    "        continue\n",
    "    \n",
    "    #format\n",
    "    stock['Date'] = pd.to_datetime(stock['Date'])\n",
    "    \n",
    "    #attach label\n",
    "    stock['Label'] = (stock['Close'] < stock.shift(periods=-1)['Open']).astype(int)\n",
    "    stock.drop(stock.tail(1).index,inplace=True)\n",
    "    \n",
    "    stock_label_count = stock[\"Label\"].value_counts()\n",
    "    \n",
    "    #ignor unblanaced stocks\n",
    "    if (abs(stock_label_count[0] - stock_label_count[1]) / stock_label_count[0]) > 0.2:\n",
    "        continue\n",
    "        \n",
    "    #attach features for each day or drop days with missing features\n",
    "    df = pd.merge(abc, stock[['Date','Label']], on='Date')\n",
    "    \n",
    "    #bag of words for headline\n",
    "    count_vect = CountVectorizer(ngram_range=(1, 1))\n",
    "    data_counts = count_vect.fit_transform(\n",
    "            df['headline_text'])\n",
    "\n",
    "    tf_transformer = TfidfTransformer(use_idf=True, norm='l2').fit(data_counts)\n",
    "    data_tf = tf_transformer.transform(data_counts)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data_tf,\n",
    "        df['Label'],\n",
    "        random_state = 42,\n",
    "        stratify = df['Label'])\n",
    "    \n",
    "    model = MultinomialNB(alpha=0.1)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    model_predict = model.predict(X_test)\n",
    "    \n",
    "    #evaluate(model_predict, y_test)\n",
    "    score = metrics.balanced_accuracy_score(y_test, model_predict)\n",
    "    \n",
    "    if best_score < score:\n",
    "        best_score = score\n",
    "    \n",
    "    print(count)\n",
    "    \n",
    "    count = count+1\n",
    "    \n",
    "    if count > 20:\n",
    "        break\n",
    "\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df['headline_text'],\n",
    "        df['Label'],\n",
    "        random_state = 42,\n",
    "        stratify = df['Label'])\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': [1, 1e-1, 1e-2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(text_clf, parameters, cv=10, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score: ', grid_search.best_score_) \n",
    "print('Best Params: ', grid_search.best_params_)\n",
    "\n",
    "\n",
    "loss = ['hinge', 'log', 'modified_huber', 'squared_hinge','perceptron']\n",
    "penalty = ['l1', 'l2', 'elasticnet'] \n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive'] \n",
    "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
    "eta0 = [1, 10, 100] \n",
    "\n",
    "param_distributions = dict(loss=loss,\n",
    "                           penalty=penalty,\n",
    "                           alpha=alpha, \n",
    "                           learning_rate=learning_rate, \n",
    "                           class_weight=class_weight, \n",
    "                           eta0=eta0) \n",
    "\n",
    "random = RandomizedSearchCV(estimator=sgd,\n",
    "                            param_distributions=param_distributions,\n",
    "                            scoring='roc_auc',\n",
    "                            verbose=1, n_jobs=-1, \n",
    "                            n_iter=100) \n",
    "\n",
    "random.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score: ', random.best_score_) \n",
    "print('Best Params: ', random.best_params_)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
